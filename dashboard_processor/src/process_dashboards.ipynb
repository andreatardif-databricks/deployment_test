{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard Processing Job\n",
    "This notebook processes Lakeview dashboards, fetches active dashboards in parallel, and merges with dashboard actions data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job parameters\n",
    "dbutils.widgets.text(\"catalog\", \"andrea_tardif\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"bronze\", \"Schema\")\n",
    "dbutils.widgets.text(\"table_name\", \"dashboards_merged\", \"Table Name\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Table Name: {table_name}\")\n",
    "\n",
    "# Construct full table path\n",
    "target_table = f\"{catalog}.{schema}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.dashboards import LifecycleState\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Workspace Client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "def fetch_name_if_active(dashboard_id: str):\n",
    "    \"\"\"\n",
    "    Returns (dashboard_id, display_name) ONLY if the dashboard exists and is ACTIVE.\n",
    "    If not ACTIVE / not found / no perms, return None so we can skip it.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        d = w.lakeview.get(dashboard_id)\n",
    "        if d.lifecycle_state != LifecycleState.ACTIVE:\n",
    "            return None\n",
    "        return {\"dashboard_id\": dashboard_id, \"display_name\": d.display_name}\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = WorkspaceClient()\n",
    "host = w.config.host.rstrip(\"/\")\n",
    "\n",
    "dashboards_actions = spark.sql(\"\"\"\n",
    "WITH actions AS (\n",
    "  SELECT\n",
    "    request_params.dashboard_id AS dashboard_id,\n",
    "    action_name,\n",
    "    user_identity.email AS actor_email,\n",
    "    identity_metadata.run_by AS run_by,\n",
    "    identity_metadata.run_as AS run_as,\n",
    "    event_time,\n",
    "    row_number() OVER (\n",
    "      PARTITION BY request_params.dashboard_id\n",
    "      ORDER BY event_time DESC\n",
    "    ) AS rn\n",
    "  FROM system.access.audit\n",
    "  WHERE event_time >= current_timestamp() - INTERVAL 180 DAYS\n",
    "    AND service_name = 'dashboards'\n",
    "    AND action_name IN ('publishDashboard','unpublishDashboard')\n",
    ")\n",
    "SELECT\n",
    "  dashboard_id,\n",
    "  CASE\n",
    "    WHEN action_name = 'publishDashboard' THEN 'published'\n",
    "    ELSE 'unpublished'\n",
    "  END AS publish_state,\n",
    "  actor_email,\n",
    "  run_by,\n",
    "  run_as,\n",
    "  event_time\n",
    "FROM actions\n",
    "WHERE rn = 1\"\"\")\n",
    "\n",
    "\n",
    "dashboards_actions = dashboards_actions.withColumn(\n",
    "    \"dashboard_url\",\n",
    "    F.concat(F.lit(f\"{host}/sql/dashboardsv3/\"), F.col(\"dashboard_id\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only IDs from dashboards_actions, and only where publish_state is published/unpublished\n",
    "dashboard_ids_limited = [\n",
    "    r.dashboard_id\n",
    "    for r in (\n",
    "        dashboards_actions\n",
    "          .select(\"dashboard_id\")\n",
    "          .distinct()\n",
    "          .collect()\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(dashboard_ids_limited)} unique dashboard IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dashboard names in parallel\n",
    "max_workers = 16\n",
    "rows = []\n",
    "\n",
    "print(f\"Fetching dashboard details with {max_workers} parallel workers...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "    futures = {ex.submit(fetch_name_if_active, did): did for did in dashboard_ids_limited}\n",
    "    for fut in as_completed(futures):\n",
    "        result = fut.result()\n",
    "        if result is not None:\n",
    "            rows.append(Row(**result))\n",
    "\n",
    "print(f\"Found {len(rows)} active dashboards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from results\n",
    "df_names_spark = spark.createDataFrame(rows)\n",
    "\n",
    "# Display sample of results\n",
    "display(df_names_spark.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with dashboards_actions\n",
    "dashboards_merged = dashboards_actions.join(df_names_spark, \"dashboard_id\", \"left\")\n",
    "\n",
    "print(f\"Merged dataset has {dashboards_merged.count()} rows\")\n",
    "display(dashboards_merged.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Unity Catalog table\n",
    "print(f\"Saving to: {target_table}\")\n",
    "\n",
    "dashboards_merged.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(target_table)\n",
    "\n",
    "print(f\"Successfully saved {dashboards_merged.count()} rows to {target_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final table info\n",
    "spark.sql(f\"DESCRIBE EXTENDED {target_table}\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "process_dashboards",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
