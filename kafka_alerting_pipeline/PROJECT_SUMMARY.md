# Kafka Alerting Pipeline - Project Summary

## ğŸ¯ Project Overview

A production-ready **Databricks Asset Bundle (DAB)** featuring a **Delta Live Tables (DLT)** pipeline that:

âœ… **Reads from Kafka streams** containing multi-client data  
âœ… **Creates per-client schemas** with Bronze/Silver/Gold tables  
âœ… **Handles errors gracefully** - continues processing on failures  
âœ… **Sends alerts** when tables fail  
âœ… **Runs on serverless compute** for auto-scaling and cost efficiency  

## ğŸ“ Project Structure

```
kafka_alerting_pipeline/
â”‚
â”œâ”€â”€ databricks.yml                      # Main bundle configuration
â”œâ”€â”€ README.md                           # Comprehensive documentation
â”œâ”€â”€ SETUP_GUIDE.md                      # Step-by-step setup instructions
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ .gitignore                          # Git ignore patterns
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ pipeline.yml                    # DLT pipeline definition
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ kafka_dlt_pipeline.ipynb        # Main pipeline logic
    â”œâ”€â”€ kafka_data_generator.ipynb      # Test data generator
    â””â”€â”€ pipeline_monitoring.ipynb       # Monitoring queries
```

## ğŸ—ï¸ Architecture

### Data Flow

```
Kafka Topic (multi-client messages)
         â†“
kafka_raw_bronze (all clients)
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“        â†“        â†“
bronze_    bronze_  bronze_  bronze_
client_001 client_002 ...    client_N
    â†“         â†“        â†“        â†“
silver_    silver_  silver_  silver_
client_001 client_002 ...    client_N
    â†“         â†“        â†“        â†“
gold_      gold_    gold_    gold_
client_001 client_002 ...    client_N
_summary   _summary          _summary
```

### Error Handling Flow

```
Any Table Error
      â†“
Try-Catch Block
      â†“
send_alert()
      â†“
pipeline_errors table
      â†“
Email Notification
```

## ğŸ¨ Key Features

### 1. Multi-Client Support
- Single Kafka topic handles multiple clients
- Automatic routing to client-specific tables
- Configurable client list

### 2. Medallion Architecture
- **Bronze**: Raw validated data per client
- **Silver**: Cleaned, enriched, deduplicated
- **Gold**: Hourly aggregated business metrics

### 3. Error Resilience
- `expect_or_drop` for data quality
- Try-catch blocks around table creation
- Centralized error logging
- Pipeline continues despite individual table failures

### 4. Alerting System
- Email notifications on pipeline failures
- Centralized `pipeline_errors` table
- Real-time health monitoring view
- Custom alert queries

### 5. Serverless Compute
- Auto-scaling based on load
- Pay only for processing time
- No cluster management

### 6. Continuous Streaming
- Real-time data ingestion
- Kafka consumer group management
- Offset tracking

## ğŸ“Š Tables Created

### Common Tables
| Table | Type | Purpose |
|-------|------|---------|
| `kafka_raw_bronze` | Streaming | Raw Kafka ingestion |
| `client_list` | Table | Unique client registry |
| `pipeline_errors` | Table | Centralized error log |
| `pipeline_health_monitor` | View | Real-time health metrics |

### Per-Client Tables (for each client_id)
| Table | Type | Purpose |
|-------|------|---------|
| `bronze_{client_id}` | Streaming | Raw validated data |
| `silver_{client_id}` | Streaming | Cleaned & enriched |
| `gold_{client_id}_summary` | Streaming | Hourly aggregations |

## ğŸ”§ Configuration Requirements

### Must Configure
1. **Kafka Settings**
   - `kafka_bootstrap_servers`: Your Kafka server
   - `kafka_topic`: Topic to subscribe to
   - `kafka_consumer_group`: Consumer group ID

2. **Unity Catalog**
   - `catalog`: Your catalog name
   - `schema`: Schema name (e.g., kafka_pipeline_dev)

3. **Alerting**
   - `alert_email`: Email for notifications

4. **Clients**
   - Update `KNOWN_CLIENTS` list in notebook

### Expected Kafka Message Format

```json
{
  "client_id": "client_001",
  "client_name": "Example Corp",
  "timestamp": "2026-02-02T10:30:00Z",
  "event_type": "transaction",
  "data": {"key": "value"},
  "amount": 150.50,
  "status": "completed",
  "metadata": {"source": "web"}
}
```

## ğŸš€ Quick Start

```bash
# 1. Update databricks.yml with your settings
# 2. Update KNOWN_CLIENTS in src/kafka_dlt_pipeline.ipynb

# 3. Validate bundle
databricks bundle validate -t dev

# 4. Deploy
databricks bundle deploy -t dev

# 5. Start pipeline
databricks bundle run kafka_client_pipeline -t dev
```

## ğŸ“ˆ Monitoring

### Via Notebook
Open `src/pipeline_monitoring.ipynb` to:
- Check pipeline health
- View data volumes by client
- Monitor data quality metrics
- Track error trends
- Inspect sample data

### Via SQL

```sql
-- Check errors
SELECT * FROM your_catalog.kafka_pipeline_dev.pipeline_errors
ORDER BY error_timestamp DESC;

-- View health
SELECT * FROM your_catalog.kafka_pipeline_dev.pipeline_health_monitor;

-- Data volume
SELECT client_id, COUNT(*) 
FROM your_catalog.kafka_pipeline_dev.kafka_raw_bronze
GROUP BY client_id;
```

## ğŸ§ª Testing

Use `src/kafka_data_generator.ipynb` to:
- Generate sample Kafka messages
- Test with edge cases
- Continuous streaming simulation
- Validate error handling

## ğŸ›¡ï¸ Error Handling Strategy

### Level 1: Data Quality Expectations
```python
@dlt.expect_or_drop("valid_client_id", "client_id = 'client_001'")
@dlt.expect_or_drop("not_null_timestamp", "event_timestamp IS NOT NULL")
```

### Level 2: Try-Catch Blocks
```python
try:
    return dlt.read_stream(...)
except Exception as e:
    send_alert(client_id, table_name, str(e))
    return spark.createDataFrame([], schema)
```

### Level 3: Central Error Logging
All errors â†’ `pipeline_errors` table â†’ Email alerts

## ğŸ“‹ Data Quality Rules

### Bronze Layer
- âœ“ Valid client_id matches expected client
- âœ“ Non-null event timestamp

### Silver Layer
- âœ“ Status in: `active`, `pending`, `completed`, `failed`
- âœ“ Amount >= 0
- âœ“ Event type not null
- âœ“ Deduplication on Kafka offset/partition

### Gold Layer
- âœ“ 1-hour window aggregations
- âœ“ Metrics: count, sum, avg, min, max
- âœ“ Event type distribution

## ğŸ” Security Features

- Serverless compute isolation
- Unity Catalog access control
- Support for Kafka authentication (SASL/SSL)
- Secrets management integration
- Audit logging

## ğŸ’° Cost Optimization

- Serverless pay-per-use model
- Auto-optimize enabled
- Efficient windowed aggregations
- Deduplication to reduce storage
- No idle cluster costs

## ğŸ“ Best Practices Implemented

âœ… Medallion architecture (Bronze/Silver/Gold)  
âœ… Data quality expectations  
âœ… Error handling and recovery  
âœ… Monitoring and alerting  
âœ… Version control ready (.gitignore)  
âœ… Comprehensive documentation  
âœ… Test data generators  
âœ… Production/Dev separation  

## ğŸ“š Files Description

### Core Files

**databricks.yml**
- Bundle configuration
- Variables definition
- Target environments (dev/prod)
- Permissions

**resources/pipeline.yml**
- DLT pipeline definition
- Serverless configuration
- Continuous mode
- Notification settings

**src/kafka_dlt_pipeline.ipynb**
- Main pipeline logic
- Bronze/Silver/Gold table definitions
- Error handling
- Alert functions

### Helper Files

**src/kafka_data_generator.ipynb**
- Sample data generation
- Kafka write utilities
- Edge case testing

**src/pipeline_monitoring.ipynb**
- Health checks
- Volume metrics
- Error analysis
- Custom alerts

### Documentation

**README.md**
- Comprehensive overview
- Architecture details
- Configuration guide
- Troubleshooting

**SETUP_GUIDE.md**
- Step-by-step instructions
- Quick start
- Common issues
- Success criteria

## ğŸ”„ Deployment Targets

### Development (`dev`)
- Mode: development
- Schema: `kafka_pipeline_dev`
- For testing and iteration

### Production (`prod`)
- Mode: production
- Schema: `kafka_pipeline_prod`
- For live processing

## ğŸ“ Support Resources

- [Databricks Asset Bundles Docs](https://docs.databricks.com/dev-tools/bundles/)
- [Delta Live Tables Docs](https://docs.databricks.com/delta-live-tables/)
- [Kafka Integration Guide](https://docs.databricks.com/structured-streaming/kafka.html)

## âœ… Validation Checklist

Before going to production:

- [ ] Kafka connectivity tested
- [ ] Unity Catalog permissions verified
- [ ] All clients configured in KNOWN_CLIENTS
- [ ] Message schema matches expectations
- [ ] Test data sent and processed successfully
- [ ] Error handling tested with bad data
- [ ] Email alerts received
- [ ] Monitoring queries working
- [ ] Performance meets requirements
- [ ] Documentation reviewed and updated

## ğŸ‰ Success Metrics

Pipeline is successful when:
- âœ… All client data flows to dedicated tables
- âœ… Bronze â†’ Silver â†’ Gold transformation works
- âœ… Errors logged but don't stop processing
- âœ… Alerts received on failures
- âœ… Data quality expectations enforced
- âœ… Monitoring shows healthy metrics

---

**Built with:** Databricks Asset Bundles, Delta Live Tables, Structured Streaming  
**Compute:** Serverless  
**Architecture:** Medallion (Bronze/Silver/Gold)  
**Status:** Production Ready âœ…

