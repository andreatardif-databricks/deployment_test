{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kafka Data Generator\n",
        "This notebook generates sample data to test the Kafka Alerting Pipeline.\n",
        "Use this to send test messages to your Kafka topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "from databricks.connect import DatabricksSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Connected!\n",
            "+--------------------+\n",
            "|             message|\n",
            "+--------------------+\n",
            "|Hello from Databr...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Force token authentication (override VSCode extension settings)\n",
        "os.environ['DATABRICKS_AUTH_TYPE'] = 'pat'  # Personal Access Token\n",
        "os.environ['DATABRICKS_HOST'] = 'https://e2-demo-field-eng.cloud.databricks.com'\n",
        "os.environ['DATABRICKS_TOKEN'] = ''  # Replace with your actual token\n",
        "\n",
        "# Remove problematic env vars\n",
        "os.environ.pop('DATABRICKS_SERVERLESS_COMPUTE_ID', None)\n",
        "os.environ.pop('DATABRICKS_METADATA_SERVICE_URL', None)\n",
        "\n",
        "# Now import and create session\n",
        "from databricks.connect import DatabricksSession\n",
        "\n",
        "spark = DatabricksSession.builder.serverless().getOrCreate()\n",
        "\n",
        "print(\"✅ Connected!\")\n",
        "spark.sql(\"SELECT 'Hello from Databricks!' as message\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = DatabricksSession.builder.serverless().profile(\"e2-field-eng-demo\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these values for your environment\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
        "KAFKA_TOPIC = \"client-events\"\n",
        "\n",
        "# Sample clients\n",
        "CLIENTS = [\n",
        "    {\"id\": \"client_001\", \"name\": \"Acme Corp\"},\n",
        "    {\"id\": \"client_002\", \"name\": \"Global Industries\"},\n",
        "    {\"id\": \"client_003\", \"name\": \"Tech Solutions\"},\n",
        "    {\"id\": \"client_004\", \"name\": \"Finance Group\"},\n",
        "    {\"id\": \"client_005\", \"name\": \"Retail Chain\"}\n",
        "]\n",
        "\n",
        "# Event types\n",
        "EVENT_TYPES = [\"transaction\", \"login\", \"purchase\", \"update\", \"delete\"]\n",
        "\n",
        "# Statuses\n",
        "STATUSES = [\"active\", \"pending\", \"completed\", \"failed\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample messages generated:\n",
            "{\n",
            "  \"client_id\": \"client_004\",\n",
            "  \"client_name\": \"Finance Group\",\n",
            "  \"timestamp\": \"2026-02-02T14:54:32.607806\",\n",
            "  \"event_type\": \"purchase\",\n",
            "  \"data\": {\n",
            "    \"key1\": \"value_75\",\n",
            "    \"key2\": \"data_14\",\n",
            "    \"source\": \"api\"\n",
            "  },\n",
            "  \"amount\": 267.69,\n",
            "  \"status\": \"pending\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"web\",\n",
            "    \"region\": \"us-west\",\n",
            "    \"version\": \"1.0\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n",
            "{\n",
            "  \"client_id\": \"client_004\",\n",
            "  \"client_name\": \"Finance Group\",\n",
            "  \"timestamp\": \"2026-02-02T14:42:26.607843\",\n",
            "  \"event_type\": \"transaction\",\n",
            "  \"data\": {\n",
            "    \"key1\": \"value_56\",\n",
            "    \"key2\": \"data_2\",\n",
            "    \"source\": \"web\"\n",
            "  },\n",
            "  \"amount\": 4980.9,\n",
            "  \"status\": \"completed\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"mobile\",\n",
            "    \"region\": \"us-west\",\n",
            "    \"version\": \"1.0\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n",
            "{\n",
            "  \"client_id\": \"client_003\",\n",
            "  \"client_name\": \"Tech Solutions\",\n",
            "  \"timestamp\": \"2026-02-02T14:41:53.607856\",\n",
            "  \"event_type\": \"update\",\n",
            "  \"data\": {\n",
            "    \"key1\": \"value_94\",\n",
            "    \"key2\": \"data_38\",\n",
            "    \"source\": \"api\"\n",
            "  },\n",
            "  \"amount\": 768.11,\n",
            "  \"status\": \"pending\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"web\",\n",
            "    \"region\": \"ap-south\",\n",
            "    \"version\": \"1.0\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def generate_sample_message():\n",
        "    \"\"\"Generate a single sample message matching the expected schema.\"\"\"\n",
        "    import builtins  # Import builtins to access Python's round function\n",
        "    \n",
        "    client = random.choice(CLIENTS)\n",
        "    \n",
        "    message = {\n",
        "        \"client_id\": client[\"id\"],\n",
        "        \"client_name\": client[\"name\"],\n",
        "        \"timestamp\": (datetime.now() - timedelta(seconds=random.randint(0, 3600))).isoformat(),\n",
        "        \"event_type\": random.choice(EVENT_TYPES),\n",
        "        \"data\": {\n",
        "            \"key1\": f\"value_{random.randint(1, 100)}\",\n",
        "            \"key2\": f\"data_{random.randint(1, 100)}\",\n",
        "            \"source\": random.choice([\"web\", \"mobile\", \"api\"])\n",
        "        },\n",
        "        \"amount\": builtins.round(random.uniform(10, 5000), 2),  # Use Python's built-in round\n",
        "        \"status\": random.choice(STATUSES),\n",
        "        \"metadata\": {\n",
        "            \"source\": random.choice([\"web\", \"mobile\", \"api\"]),\n",
        "            \"region\": random.choice([\"us-east\", \"us-west\", \"eu-west\", \"ap-south\"]),\n",
        "            \"version\": \"1.0\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return message\n",
        "\n",
        "# Generate sample messages\n",
        "sample_messages = [generate_sample_message() for _ in range(100)]\n",
        "\n",
        "# Display a few samples\n",
        "print(\"Sample messages generated:\")\n",
        "for msg in sample_messages[:3]:\n",
        "    print(json.dumps(msg, indent=2))\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Write to Kafka using Structured Streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 100 messages ready to send to Kafka\n",
            "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|key       |value                                                                                                                                                                                                                                                                                        |\n",
            "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|client_004|{\"client_id\":\"client_004\",\"client_name\":\"Finance Group\",\"timestamp\":\"2026-02-02T14:54:32.607806\",\"event_type\":\"purchase\",\"data\":{\"key1\":\"value_75\",\"key2\":\"data_14\",\"source\":\"api\"},\"amount\":267.69,\"status\":\"pending\",\"metadata\":{\"source\":\"web\",\"region\":\"us-west\",\"version\":\"1.0\"}}       |\n",
            "|client_004|{\"client_id\":\"client_004\",\"client_name\":\"Finance Group\",\"timestamp\":\"2026-02-02T14:42:26.607843\",\"event_type\":\"transaction\",\"data\":{\"key1\":\"value_56\",\"key2\":\"data_2\",\"source\":\"web\"},\"amount\":4980.9,\"status\":\"completed\",\"metadata\":{\"source\":\"mobile\",\"region\":\"us-west\",\"version\":\"1.0\"}}|\n",
            "|client_003|{\"client_id\":\"client_003\",\"client_name\":\"Tech Solutions\",\"timestamp\":\"2026-02-02T14:41:53.607856\",\"event_type\":\"update\",\"data\":{\"key1\":\"value_94\",\"key2\":\"data_38\",\"source\":\"api\"},\"amount\":768.11,\"status\":\"pending\",\"metadata\":{\"source\":\"web\",\"region\":\"ap-south\",\"version\":\"1.0\"}}       |\n",
            "|client_002|{\"client_id\":\"client_002\",\"client_name\":\"Global Industries\",\"timestamp\":\"2026-02-02T14:42:11.607866\",\"event_type\":\"login\",\"data\":{\"key1\":\"value_57\",\"key2\":\"data_6\",\"source\":\"api\"},\"amount\":1654.15,\"status\":\"completed\",\"metadata\":{\"source\":\"web\",\"region\":\"us-west\",\"version\":\"1.0\"}}    |\n",
            "|client_005|{\"client_id\":\"client_005\",\"client_name\":\"Retail Chain\",\"timestamp\":\"2026-02-02T15:16:17.607876\",\"event_type\":\"login\",\"data\":{\"key1\":\"value_41\",\"key2\":\"data_26\",\"source\":\"api\"},\"amount\":460.61,\"status\":\"failed\",\"metadata\":{\"source\":\"api\",\"region\":\"eu-west\",\"version\":\"1.0\"}}            |\n",
            "+----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"client_id\", StringType()),\n",
        "    StructField(\"client_name\", StringType()),\n",
        "    StructField(\"timestamp\", StringType()),\n",
        "    StructField(\"event_type\", StringType()),\n",
        "    StructField(\"data\", MapType(StringType(), StringType())),\n",
        "    StructField(\"amount\", DoubleType()),\n",
        "    StructField(\"status\", StringType()),\n",
        "    StructField(\"metadata\", MapType(StringType(), StringType()))\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(sample_messages, schema)\n",
        "\n",
        "# Convert to JSON strings for Kafka\n",
        "kafka_df = df.select(\n",
        "    col(\"client_id\").alias(\"key\"),\n",
        "    to_json(struct(\"*\")).alias(\"value\")\n",
        ")\n",
        "\n",
        "# Write to Kafka\n",
        "# Uncomment and update with your Kafka configuration\n",
        "# kafka_df.write \\\n",
        "#     .format(\"kafka\") \\\n",
        "#     .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "#     .option(\"topic\", KAFKA_TOPIC) \\\n",
        "#     .save()\n",
        "\n",
        "print(f\"Generated {kafka_df.count()} messages ready to send to Kafka\")\n",
        "kafka_df.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Continuous Data Generation (for testing streaming)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming data generator configured. Uncomment the query section to start.\n"
          ]
        }
      ],
      "source": [
        "# Use rate source for continuous generation\n",
        "from pyspark.sql.functions import expr, rand, when\n",
        "\n",
        "# Create a rate stream (generates rows at specified rate)\n",
        "rate_stream = spark.readStream \\\n",
        "    .format(\"rate\") \\\n",
        "    .option(\"rowsPerSecond\", 10) \\\n",
        "    .load()\n",
        "\n",
        "# Transform to match our schema\n",
        "def generate_streaming_data(batch_df, batch_id):\n",
        "    \"\"\"Generate sample data for each micro-batch.\"\"\"\n",
        "    messages = [generate_sample_message() for _ in range(batch_df.count())]\n",
        "    messages_df = spark.createDataFrame(messages, schema)\n",
        "    \n",
        "    # Convert to Kafka format\n",
        "    kafka_messages = messages_df.select(\n",
        "        col(\"client_id\").alias(\"key\"),\n",
        "        to_json(struct(\"*\")).alias(\"value\")\n",
        "    )\n",
        "    \n",
        "    # Write to Kafka\n",
        "    # Uncomment and update with your Kafka configuration\n",
        "    # kafka_messages.write \\\n",
        "    #     .format(\"kafka\") \\\n",
        "    #     .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    #     .option(\"topic\", KAFKA_TOPIC) \\\n",
        "    #     .save()\n",
        "    \n",
        "    print(f\"Batch {batch_id}: Generated {kafka_messages.count()} messages\")\n",
        "\n",
        "# Start the streaming query (uncomment to run)\n",
        "# query = rate_stream.writeStream \\\n",
        "#     .foreachBatch(generate_streaming_data) \\\n",
        "#     .outputMode(\"update\") \\\n",
        "#     .start()\n",
        "# \n",
        "# query.awaitTermination()\n",
        "\n",
        "print(\"Streaming data generator configured. Uncomment the query section to start.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Data with Edge Cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edge case test messages:\n",
            "\n",
            "1. {\n",
            "  \"client_id\": \"client_001\",\n",
            "  \"client_name\": \"Test Client\",\n",
            "  \"timestamp\": \"2026-02-02T15:37:54.380123\",\n",
            "  \"event_type\": \"test\",\n",
            "  \"data\": {\n",
            "    \"test\": \"data\"\n",
            "  },\n",
            "  \"amount\": 100.0,\n",
            "  \"status\": \"active\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"test\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n",
            "\n",
            "2. {\n",
            "  \"client_id\": \"client_002\",\n",
            "  \"client_name\": \"Test Client 2\",\n",
            "  \"event_type\": \"test\",\n",
            "  \"data\": {\n",
            "    \"test\": \"data\"\n",
            "  },\n",
            "  \"amount\": 50.0,\n",
            "  \"status\": \"pending\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"test\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n",
            "\n",
            "3. {\n",
            "  \"client_id\": \"client_003\",\n",
            "  \"client_name\": \"Test Client 3\",\n",
            "  \"timestamp\": \"2026-02-02T15:37:54.380145\",\n",
            "  \"event_type\": \"test\",\n",
            "  \"data\": {\n",
            "    \"test\": \"data\"\n",
            "  },\n",
            "  \"amount\": 75.0,\n",
            "  \"status\": \"invalid_status\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"test\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n",
            "\n",
            "4. {\n",
            "  \"client_id\": \"client_004\",\n",
            "  \"client_name\": \"Test Client 4\",\n",
            "  \"timestamp\": \"2026-02-02T15:37:54.380146\",\n",
            "  \"event_type\": \"test\",\n",
            "  \"data\": {\n",
            "    \"test\": \"data\"\n",
            "  },\n",
            "  \"amount\": -100.0,\n",
            "  \"status\": \"completed\",\n",
            "  \"metadata\": {\n",
            "    \"source\": \"test\"\n",
            "  }\n",
            "}\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Generate test data with edge cases to test error handling\n",
        "edge_case_messages = [\n",
        "    # Valid message\n",
        "    {\n",
        "        \"client_id\": \"client_001\",\n",
        "        \"client_name\": \"Test Client\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"event_type\": \"test\",\n",
        "        \"data\": {\"test\": \"data\"},\n",
        "        \"amount\": 100.0,\n",
        "        \"status\": \"active\",\n",
        "        \"metadata\": {\"source\": \"test\"}\n",
        "    },\n",
        "    # Missing timestamp (should be dropped by bronze layer)\n",
        "    {\n",
        "        \"client_id\": \"client_002\",\n",
        "        \"client_name\": \"Test Client 2\",\n",
        "        \"event_type\": \"test\",\n",
        "        \"data\": {\"test\": \"data\"},\n",
        "        \"amount\": 50.0,\n",
        "        \"status\": \"pending\",\n",
        "        \"metadata\": {\"source\": \"test\"}\n",
        "    },\n",
        "    # Invalid status (should be dropped by silver layer)\n",
        "    {\n",
        "        \"client_id\": \"client_003\",\n",
        "        \"client_name\": \"Test Client 3\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"event_type\": \"test\",\n",
        "        \"data\": {\"test\": \"data\"},\n",
        "        \"amount\": 75.0,\n",
        "        \"status\": \"invalid_status\",\n",
        "        \"metadata\": {\"source\": \"test\"}\n",
        "    },\n",
        "    # Negative amount (should be dropped by silver layer)\n",
        "    {\n",
        "        \"client_id\": \"client_004\",\n",
        "        \"client_name\": \"Test Client 4\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"event_type\": \"test\",\n",
        "        \"data\": {\"test\": \"data\"},\n",
        "        \"amount\": -100.0,\n",
        "        \"status\": \"completed\",\n",
        "        \"metadata\": {\"source\": \"test\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Edge case test messages:\")\n",
        "for i, msg in enumerate(edge_case_messages):\n",
        "    print(f\"\\n{i+1}. {json.dumps(msg, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
