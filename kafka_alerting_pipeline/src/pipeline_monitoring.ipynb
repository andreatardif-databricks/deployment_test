{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pipeline Monitoring Dashboard\n",
        "This notebook provides queries to monitor the Kafka Alerting Pipeline health and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update with your catalog and schema\n",
        "CATALOG = \"andrea_tardif\"\n",
        "SCHEMA = \"kafka_pipeline_dev\"\n",
        "\n",
        "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
        "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Overall Pipeline Health\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check recent errors\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    table_name,\n",
        "    COUNT(*) as error_count,\n",
        "    MAX(error_timestamp) as last_error,\n",
        "    COLLECT_LIST(error_message) as error_messages\n",
        "FROM {CATALOG}.{SCHEMA}.pipeline_errors\n",
        "WHERE error_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
        "GROUP BY client_id, table_name\n",
        "ORDER BY error_count DESC\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Volume by Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Volume by client in the last hour\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    client_name,\n",
        "    COUNT(*) as message_count,\n",
        "    MIN(ingestion_timestamp) as first_message,\n",
        "    MAX(ingestion_timestamp) as last_message\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "WHERE ingestion_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
        "GROUP BY client_id, client_name\n",
        "ORDER BY message_count DESC\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare record counts across layers (for client_001 as example)\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "try:\n",
        "    bronze_count = spark.table(f\"{CATALOG}.{SCHEMA}.bronze_client_001\").count()\n",
        "    silver_count = spark.table(f\"{CATALOG}.{SCHEMA}.silver_client_001\").count()\n",
        "    gold_count = spark.table(f\"{CATALOG}.{SCHEMA}.gold_client_001_summary\").count()\n",
        "    \n",
        "    quality_df = spark.createDataFrame([\n",
        "        (\"Bronze\", bronze_count),\n",
        "        (\"Silver\", silver_count),\n",
        "        (\"Gold\", gold_count)\n",
        "    ], [\"Layer\", \"Record Count\"])\n",
        "    \n",
        "    quality_df.display()\n",
        "    \n",
        "    # Calculate drop rates\n",
        "    bronze_to_silver_drop = ((bronze_count - silver_count) / bronze_count * 100) if bronze_count > 0 else 0\n",
        "    print(f\"\\nDrop rate from Bronze to Silver: {bronze_to_silver_drop:.2f}%\")\n",
        "except Exception as e:\n",
        "    print(f\"Error calculating quality metrics: {str(e)}\")\n",
        "    print(\"Ensure tables exist and have data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Event Type Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Event type distribution by client\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    event_type,\n",
        "    COUNT(*) as event_count,\n",
        "    ROUND(AVG(amount), 2) as avg_amount,\n",
        "    ROUND(SUM(amount), 2) as total_amount\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "WHERE ingestion_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
        "GROUP BY client_id, event_type\n",
        "ORDER BY client_id, event_count DESC\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Gold Layer Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View latest gold metrics for client_001\n",
        "try:\n",
        "    spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        client_id,\n",
        "        window_start,\n",
        "        window_end,\n",
        "        event_type,\n",
        "        status,\n",
        "        event_count,\n",
        "        ROUND(total_amount, 2) as total_amount,\n",
        "        ROUND(avg_amount, 2) as avg_amount,\n",
        "        min_amount,\n",
        "        max_amount\n",
        "    FROM {CATALOG}.{SCHEMA}.gold_client_001_summary\n",
        "    ORDER BY window_start DESC\n",
        "    LIMIT 20\n",
        "    \"\"\").display()\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(\"Gold table may not exist yet or may not have data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Kafka Offset Monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Track Kafka offset progress\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    kafka_partition,\n",
        "    MIN(kafka_offset) as min_offset,\n",
        "    MAX(kafka_offset) as max_offset,\n",
        "    COUNT(*) as messages_processed,\n",
        "    MAX(ingestion_timestamp) as last_ingestion_time\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "GROUP BY kafka_partition\n",
        "ORDER BY kafka_partition\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Client Status Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Status distribution by client\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    status,\n",
        "    COUNT(*) as status_count,\n",
        "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY client_id), 2) as percentage\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "WHERE ingestion_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
        "GROUP BY client_id, status\n",
        "ORDER BY client_id, status_count DESC\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Historical Error Trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error trends over the last 24 hours\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    DATE_TRUNC('hour', error_timestamp) as error_hour,\n",
        "    client_id,\n",
        "    COUNT(*) as error_count\n",
        "FROM {CATALOG}.{SCHEMA}.pipeline_errors\n",
        "WHERE error_timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
        "GROUP BY DATE_TRUNC('hour', error_timestamp), client_id\n",
        "ORDER BY error_hour DESC, error_count DESC\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Sample Data Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View recent messages\n",
        "spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    client_name,\n",
        "    event_timestamp,\n",
        "    event_type,\n",
        "    amount,\n",
        "    status,\n",
        "    ingestion_timestamp\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "ORDER BY ingestion_timestamp DESC\n",
        "LIMIT 20\n",
        "\"\"\").display()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Custom Alert Query\n",
        "Create custom alerts based on business rules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Alert if any client has > 10 failed transactions in the last hour\n",
        "alerts = spark.sql(f\"\"\"\n",
        "SELECT \n",
        "    client_id,\n",
        "    COUNT(*) as failed_count,\n",
        "    MAX(event_timestamp) as last_failure\n",
        "FROM {CATALOG}.{SCHEMA}.kafka_raw_bronze\n",
        "WHERE status = 'failed'\n",
        "  AND ingestion_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
        "GROUP BY client_id\n",
        "HAVING COUNT(*) > 10\n",
        "ORDER BY failed_count DESC\n",
        "\"\"\")\n",
        "\n",
        "if alerts.count() > 0:\n",
        "    print(\"⚠️  ALERT: The following clients have high failure rates:\")\n",
        "    alerts.display()\n",
        "else:\n",
        "    print(\"✅ No alerts - all clients within acceptable failure thresholds\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
