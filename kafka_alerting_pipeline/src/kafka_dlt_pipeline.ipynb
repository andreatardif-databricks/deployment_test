{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kafka Multi-Client DLT Pipeline\n",
        "This pipeline reads from a Kafka stream containing data from multiple clients and creates:\n",
        "- A separate schema per client\n",
        "- Bronze, Silver, and Gold tables for each client\n",
        "- Error handling that continues processing on failures\n",
        "- Alerting on failures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dlt\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get configuration from pipeline settings\n",
        "kafka_bootstrap_servers = spark.conf.get(\"kafka.bootstrap.servers\")\n",
        "kafka_topic = spark.conf.get(\"kafka.topic\")\n",
        "alert_email = spark.conf.get(\"alert.email\", \"\")\n",
        "catalog = spark.conf.get(\"catalog\")\n",
        "base_schema = spark.conf.get(\"base_schema\")\n",
        "\n",
        "print(f\"Pipeline Configuration:\")\n",
        "print(f\"  Kafka Servers: {kafka_bootstrap_servers}\")\n",
        "print(f\"  Kafka Topic: {kafka_topic}\")\n",
        "print(f\"  Catalog: {catalog}\")\n",
        "print(f\"  Base Schema: {base_schema}\")\n",
        "print(f\"  Note: Consumer group ID not used (not supported on serverless)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected schema for incoming Kafka messages\n",
        "# Adjust this schema based on your actual Kafka message structure\n",
        "message_schema = StructType([\n",
        "    StructField(\"client_id\", StringType(), False),\n",
        "    StructField(\"client_name\", StringType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"data\", MapType(StringType(), StringType()), True),\n",
        "    StructField(\"amount\", DoubleType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"metadata\", MapType(StringType(), StringType()), True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to send alerts\n",
        "def send_alert(client_id, table_name, error_message):\n",
        "    \"\"\"\n",
        "    Log errors and prepare for alerting.\n",
        "    In production, this could integrate with external alerting systems.\n",
        "    \"\"\"\n",
        "    alert_msg = f\"\"\"Pipeline Alert:\n",
        "    Client: {client_id}\n",
        "    Table: {table_name}\n",
        "    Error: {error_message}\n",
        "    Timestamp: {datetime.now().isoformat()}\n",
        "    \"\"\"\n",
        "    logger.error(alert_msg)\n",
        "    \n",
        "    # Log to a central error tracking table\n",
        "    try:\n",
        "        error_df = spark.createDataFrame([\n",
        "            (client_id, table_name, error_message, datetime.now())\n",
        "        ], [\"client_id\", \"table_name\", \"error_message\", \"error_timestamp\"])\n",
        "        \n",
        "        error_df.write.mode(\"append\").saveAsTable(f\"{catalog}.{base_schema}.pipeline_errors\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to log error to table: {str(e)}\")\n",
        "    \n",
        "    return alert_msg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bronze Layer - Raw Kafka Ingestion\n",
        "Read raw data from Kafka stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dlt.table(\n",
        "    name=\"kafka_raw_bronze\",\n",
        "    comment=\"Raw data from Kafka stream - all clients\",\n",
        "    table_properties={\n",
        "        \"quality\": \"bronze\",\n",
        "        \"pipelines.autoOptimize.managed\": \"true\"\n",
        "    }\n",
        ")\n",
        "def kafka_raw_bronze():\n",
        "    \"\"\"\n",
        "    Read raw data from Kafka and parse JSON messages.\n",
        "    This is the entry point for all client data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return (\n",
        "            spark.readStream\n",
        "                .format(\"kafka\")\n",
        "                .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
        "                .option(\"subscribe\", kafka_topic)\n",
        "                .option(\"startingOffsets\", \"earliest\")  # Changed from 'latest' to read all messages\n",
        "                .option(\"failOnDataLoss\", \"false\")  # Continue on data loss\n",
        "                # Note: kafka.consumer.group.id is NOT supported on serverless/shared clusters\n",
        "                .load()\n",
        "                .select(\n",
        "                    col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
        "                    col(\"value\").cast(\"string\").alias(\"kafka_value\"),\n",
        "                    col(\"topic\").alias(\"kafka_topic\"),\n",
        "                    col(\"partition\").alias(\"kafka_partition\"),\n",
        "                    col(\"offset\").alias(\"kafka_offset\"),\n",
        "                    col(\"timestamp\").alias(\"kafka_timestamp\")\n",
        "                )\n",
        "                .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
        "                .withColumn(\n",
        "                    \"parsed_value\",\n",
        "                    from_json(col(\"kafka_value\"), message_schema)\n",
        "                )\n",
        "                .select(\n",
        "                    \"kafka_key\",\n",
        "                    \"kafka_value\",\n",
        "                    \"kafka_topic\",\n",
        "                    \"kafka_partition\",\n",
        "                    \"kafka_offset\",\n",
        "                    \"kafka_timestamp\",\n",
        "                    \"ingestion_timestamp\",\n",
        "                    col(\"parsed_value.client_id\").alias(\"client_id\"),\n",
        "                    col(\"parsed_value.client_name\").alias(\"client_name\"),\n",
        "                    col(\"parsed_value.timestamp\").alias(\"event_timestamp\"),\n",
        "                    col(\"parsed_value.event_type\").alias(\"event_type\"),\n",
        "                    col(\"parsed_value.data\").alias(\"data\"),\n",
        "                    col(\"parsed_value.amount\").alias(\"amount\"),\n",
        "                    col(\"parsed_value.status\").alias(\"status\"),\n",
        "                    col(\"parsed_value.metadata\").alias(\"metadata\")\n",
        "                )\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in kafka_raw_bronze: {str(e)}\")\n",
        "        send_alert(\"ALL\", \"kafka_raw_bronze\", str(e))\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Client List Table\n",
        "Maintain a list of all unique clients seen in the stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dlt.table(\n",
        "    name=\"client_list\",\n",
        "    comment=\"List of all unique clients in the system\"\n",
        ")\n",
        "def client_list():\n",
        "    \"\"\"\n",
        "    Track all unique clients that have been seen.\n",
        "    This helps with schema management and monitoring.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        dlt.read_stream(\"kafka_raw_bronze\")\n",
        "            .select(\n",
        "                \"client_id\",\n",
        "                \"client_name\"\n",
        "            )\n",
        "            .dropDuplicates([\"client_id\"])\n",
        "            .withColumn(\"first_seen\", current_timestamp())\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Per-Client Bronze Tables\n",
        "Create bronze tables for each client with error handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_client_bronze_table(client_id):\n",
        "    \"\"\"\n",
        "    Create a bronze table for a specific client.\n",
        "    Uses expect_or_drop to handle malformed data gracefully.\n",
        "    \"\"\"\n",
        "    table_name = f\"bronze_{client_id}\"\n",
        "    \n",
        "    @dlt.table(\n",
        "        name=table_name,\n",
        "        comment=f\"Bronze layer for client {client_id} - Raw validated data\",\n",
        "        table_properties={\n",
        "            \"quality\": \"bronze\",\n",
        "            \"client_id\": client_id\n",
        "        }\n",
        "    )\n",
        "    @dlt.expect_or_drop(\"valid_client_id\", f\"client_id = '{client_id}'\")\n",
        "    @dlt.expect_or_drop(\"not_null_timestamp\", \"event_timestamp IS NOT NULL\")\n",
        "    def bronze_table():\n",
        "        try:\n",
        "            return (\n",
        "                dlt.read_stream(\"kafka_raw_bronze\")\n",
        "                    .filter(col(\"client_id\") == client_id)\n",
        "                    .withColumn(\"bronze_processed_timestamp\", current_timestamp())\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating bronze table for client {client_id}: {str(e)}\")\n",
        "            send_alert(client_id, table_name, str(e))\n",
        "            # Return empty dataframe to allow pipeline to continue\n",
        "            return spark.createDataFrame([], schema=\"client_id STRING\")\n",
        "    \n",
        "    return bronze_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Per-Client Silver Tables\n",
        "Create silver tables with data quality checks and transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def create_client_silver_table(client_id):\n",
        "#     \"\"\"\n",
        "#     Create a silver table for a specific client.\n",
        "#     Applies data quality rules and enrichment.\n",
        "#     \"\"\"\n",
        "#     bronze_table_name = f\"bronze_{client_id}\"\n",
        "#     silver_table_name = f\"silver_{client_id}\"\n",
        "    \n",
        "#     @dlt.table(\n",
        "#         name=silver_table_name,\n",
        "#         comment=f\"Silver layer for client {client_id} - Cleaned and enriched data\",\n",
        "#         table_properties={\n",
        "#             \"quality\": \"silver\",\n",
        "#             \"client_id\": client_id\n",
        "#         }\n",
        "#     )\n",
        "#     @dlt.expect_or_drop(\"valid_status\", \"status IN ('active', 'pending', 'completed', 'failed')\")\n",
        "#     @dlt.expect_or_drop(\"valid_amount\", \"amount >= 0\")\n",
        "#     @dlt.expect(\"has_event_type\", \"event_type IS NOT NULL\")\n",
        "#     def silver_table():\n",
        "#         try:\n",
        "#             return (\n",
        "#                 dlt.read_stream(bronze_table_name)\n",
        "#                     .select(\n",
        "#                         \"client_id\",\n",
        "#                         \"client_name\",\n",
        "#                         \"event_timestamp\",\n",
        "#                         \"event_type\",\n",
        "#                         \"amount\",\n",
        "#                         \"status\",\n",
        "#                         \"data\",\n",
        "#                         \"metadata\",\n",
        "#                         \"kafka_offset\",\n",
        "#                         \"kafka_partition\"\n",
        "#                     )\n",
        "#                     # Add enrichment columns\n",
        "#                     .withColumn(\"silver_processed_timestamp\", current_timestamp())\n",
        "#                     .withColumn(\"year\", year(col(\"event_timestamp\")))\n",
        "#                     .withColumn(\"month\", month(col(\"event_timestamp\")))\n",
        "#                     .withColumn(\"day\", dayofmonth(col(\"event_timestamp\")))\n",
        "#                     .withColumn(\n",
        "#                         \"amount_category\",\n",
        "#                         when(col(\"amount\") < 100, \"small\")\n",
        "#                         .when(col(\"amount\") < 1000, \"medium\")\n",
        "#                         .otherwise(\"large\")\n",
        "#                     )\n",
        "#                     # Deduplicate based on kafka offset and partition\n",
        "#                     .dropDuplicates([\"kafka_offset\", \"kafka_partition\"])\n",
        "#             )\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Error creating silver table for client {client_id}: {str(e)}\")\n",
        "#             send_alert(client_id, silver_table_name, str(e))\n",
        "#             # Return empty dataframe to allow pipeline to continue\n",
        "#             return spark.createDataFrame([], schema=\"client_id STRING\")\n",
        "    \n",
        "#     return silver_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def create_client_gold_table(client_id):\n",
        "#     \"\"\"\n",
        "#     Create a gold table for a specific client.\n",
        "#     Contains aggregated business metrics.\n",
        "#     \"\"\"\n",
        "#     silver_table_name = f\"silver_{client_id}\"\n",
        "#     gold_table_name = f\"gold_{client_id}_summary\"\n",
        "    \n",
        "#     @dlt.table(\n",
        "#         name=gold_table_name,\n",
        "#         comment=f\"Gold layer for client {client_id} - Aggregated metrics\",\n",
        "#         table_properties={\n",
        "#             \"quality\": \"gold\",\n",
        "#             \"client_id\": client_id\n",
        "#         }\n",
        "#     )\n",
        "#     def gold_table():\n",
        "#         try:\n",
        "#             return (\n",
        "#                 dlt.read_stream(silver_table_name)\n",
        "#                     .groupBy(\n",
        "#                         \"client_id\",\n",
        "#                         \"client_name\",\n",
        "#                         window(col(\"event_timestamp\"), \"1 hour\"),\n",
        "#                         \"event_type\",\n",
        "#                         \"status\"\n",
        "#                     )\n",
        "#                     .agg(\n",
        "#                         count(\"*\").alias(\"event_count\"),\n",
        "#                         sum(\"amount\").alias(\"total_amount\"),\n",
        "#                         avg(\"amount\").alias(\"avg_amount\"),\n",
        "#                         min(\"amount\").alias(\"min_amount\"),\n",
        "#                         max(\"amount\").alias(\"max_amount\"),\n",
        "#                         countDistinct(\"event_type\").alias(\"unique_event_types\")\n",
        "#                     )\n",
        "#                     .select(\n",
        "#                         \"client_id\",\n",
        "#                         \"client_name\",\n",
        "#                         col(\"window.start\").alias(\"window_start\"),\n",
        "#                         col(\"window.end\").alias(\"window_end\"),\n",
        "#                         \"event_type\",\n",
        "#                         \"status\",\n",
        "#                         \"event_count\",\n",
        "#                         \"total_amount\",\n",
        "#                         \"avg_amount\",\n",
        "#                         \"min_amount\",\n",
        "#                         \"max_amount\",\n",
        "#                         \"unique_event_types\"\n",
        "#                     )\n",
        "#                     .withColumn(\"gold_processed_timestamp\", current_timestamp())\n",
        "#             )\n",
        "#         except Exception as e:\n",
        "#             logger.error(f\"Error creating gold table for client {client_id}: {str(e)}\")\n",
        "#             send_alert(client_id, gold_table_name, str(e))\n",
        "#             # Return empty dataframe to allow pipeline to continue\n",
        "#             return spark.createDataFrame([], schema=\"client_id STRING\")\n",
        "    \n",
        "#     return gold_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Tables for Known Clients\n",
        "This section dynamically creates tables for each client.\n",
        "In production, you might want to:\n",
        "1. Read the list of clients from a configuration table\n",
        "2. Auto-discover clients from the stream\n",
        "3. Use a scheduled job to add new clients as they appear\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Example: Define your clients here or read from a configuration\n",
        "# # For demonstration, we'll create tables for a few example clients\n",
        "# # In production, you would dynamically discover these from the stream\n",
        "\n",
        "# KNOWN_CLIENTS = [\n",
        "#     \"client_001\",\n",
        "#     \"client_002\",\n",
        "#     \"client_003\",\n",
        "#     \"client_004\",\n",
        "#     \"client_005\"\n",
        "# ]\n",
        "\n",
        "# # Create bronze, silver, and gold tables for each known client\n",
        "# for client_id in KNOWN_CLIENTS:\n",
        "#     try:\n",
        "#         # Create and register bronze table\n",
        "#         bronze_func = create_client_bronze_table(client_id)\n",
        "#         globals()[f\"bronze_{client_id}\"] = bronze_func()  # Call the returned function\n",
        "        \n",
        "#         # Create and register silver table\n",
        "#         silver_func = create_client_silver_table(client_id)\n",
        "#         globals()[f\"silver_{client_id}\"] = silver_func()  # Call the returned function\n",
        "        \n",
        "#         # Create and register gold table\n",
        "#         gold_func = create_client_gold_table(client_id)\n",
        "#         globals()[f\"gold_{client_id}_summary\"] = gold_func()  # Call the returned function\n",
        "        \n",
        "#         logger.info(f\"Successfully initialized tables for client: {client_id}\")\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Failed to initialize tables for client {client_id}: {str(e)}\")\n",
        "#         send_alert(client_id, \"initialization\", str(e))\n",
        "#         # Continue with next client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Tracking Table\n",
        "Create a table to track all pipeline errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @dlt.table(\n",
        "#     name=\"pipeline_errors\",\n",
        "#     comment=\"Centralized error tracking for the pipeline\"\n",
        "# )\n",
        "# def pipeline_errors():\n",
        "#     \"\"\"\n",
        "#     This table captures all errors that occur during pipeline execution.\n",
        "#     Use this for monitoring and alerting.\n",
        "#     \"\"\"\n",
        "#     # This table is populated by the send_alert function\n",
        "#     # It's created as an empty table that will be populated via append mode\n",
        "#     schema = StructType([\n",
        "#         StructField(\"client_id\", StringType(), False),\n",
        "#         StructField(\"table_name\", StringType(), False),\n",
        "#         StructField(\"error_message\", StringType(), False),\n",
        "#         StructField(\"error_timestamp\", TimestampType(), False)\n",
        "#     ])\n",
        "#     return spark.createDataFrame([], schema)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Monitoring View\n",
        "Create a view to monitor pipeline health\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @dlt.view(\n",
        "#     name=\"pipeline_health_monitor\",\n",
        "#     comment=\"Real-time view of pipeline health by client\"\n",
        "# )\n",
        "# def pipeline_health_monitor():\n",
        "#     \"\"\"\n",
        "#     Monitor the health of the pipeline by tracking:\n",
        "#     - Recent error counts per client\n",
        "#     - Last successful processing time\n",
        "#     - Data volume metrics\n",
        "#     \"\"\"\n",
        "#     return (\n",
        "#         dlt.read(\"pipeline_errors\")\n",
        "#             .filter(col(\"error_timestamp\") >= expr(\"current_timestamp() - INTERVAL 1 HOUR\"))\n",
        "#             .groupBy(\"client_id\")\n",
        "#             .agg(\n",
        "#                 count(\"*\").alias(\"error_count_last_hour\"),\n",
        "#                 max(\"error_timestamp\").alias(\"last_error_time\"),\n",
        "#                 collect_list(\"error_message\").alias(\"recent_errors\")\n",
        "#             )\n",
        "#     )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
