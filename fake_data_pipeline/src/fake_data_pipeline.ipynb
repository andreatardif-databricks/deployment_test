{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake Data Pipeline (Bronze → Silver → Gold)\n",
        "\n",
        "Generates fake data, writes bronze/silver/gold tables to Unity Catalog, and stores run metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Widgets (passed via job base_parameters)\n",
        "dbutils.widgets.text(\"catalog\", \"andrea_tardif\", \"Catalog\")\n",
        "dbutils.widgets.text(\"job_id\", \"\", \"Job ID\")\n",
        "dbutils.widgets.text(\"run_id\", \"\", \"Run ID\")\n",
        "dbutils.widgets.text(\"start_time\", \"\", \"Start time (ms)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
        ")\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "fake = Faker()\n",
        "\n",
        "CATALOG = dbutils.widgets.get(\"catalog\")\n",
        "NUM_RECORDS = 10000\n",
        "JOB_ID = dbutils.widgets.get(\"job_id\")\n",
        "RUN_ID = dbutils.widgets.get(\"run_id\")\n",
        "RUN_START = datetime.utcnow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for schema in [\"bronze\", \"silver\", \"gold\"]:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{schema}\")\n",
        "    print(f\"[INFO] Schema {CATALOG}.{schema} is ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BRONZE — raw fake customer orders\n",
        "t0 = time.time()\n",
        "\n",
        "records = [\n",
        "    (\n",
        "        fake.uuid4(),\n",
        "        fake.name(),\n",
        "        fake.email(),\n",
        "        fake.state(),\n",
        "        float(round(fake.pyfloat(min_value=5, max_value=500, right_digits=2), 2)),\n",
        "        fake.random_element([\"electronics\", \"clothing\", \"food\", \"books\", \"sports\"]),\n",
        "        fake.date_time_between(start_date=\"-90d\", end_date=\"now\"),\n",
        "        fake.random_element([\"completed\", \"pending\", \"cancelled\", \"refunded\"]),\n",
        "    )\n",
        "    for _ in range(NUM_RECORDS)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), False),\n",
        "    StructField(\"customer_name\", StringType(), False),\n",
        "    StructField(\"email\", StringType(), False),\n",
        "    StructField(\"state\", StringType(), False),\n",
        "    StructField(\"order_amount\", FloatType(), False),\n",
        "    StructField(\"category\", StringType(), False),\n",
        "    StructField(\"order_ts\", TimestampType(), False),\n",
        "    StructField(\"status\", StringType(), False),\n",
        "])\n",
        "\n",
        "bronze_df = spark.createDataFrame(records, schema=schema).withColumn(\n",
        "    \"_ingested_at\", F.current_timestamp()\n",
        ")\n",
        "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.bronze.orders\")\n",
        "bronze_rows = bronze_df.count()\n",
        "bronze_time = time.time() - t0\n",
        "print(f\"[BRONZE] {bronze_rows:,} rows written in {bronze_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SILVER — cleaned, deduplicated\n",
        "t0 = time.time()\n",
        "\n",
        "silver_df = (\n",
        "    spark.table(f\"{CATALOG}.bronze.orders\")\n",
        "    .dropDuplicates([\"order_id\"])\n",
        "    .filter(F.col(\"status\") != \"cancelled\")\n",
        "    .withColumn(\"order_amount_usd\", F.round(F.col(\"order_amount\"), 2))\n",
        "    .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
        "    .drop(\"_ingested_at\")\n",
        "    .withColumn(\"_processed_at\", F.current_timestamp())\n",
        ")\n",
        "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.silver.orders_clean\")\n",
        "silver_rows = silver_df.count()\n",
        "silver_time = time.time() - t0\n",
        "print(f\"[SILVER] {silver_rows:,} rows written in {silver_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GOLD — aggregated revenue\n",
        "t0 = time.time()\n",
        "\n",
        "gold_df = (\n",
        "    silver_df\n",
        "    .groupBy(\"category\", \"state\", \"order_date\")\n",
        "    .agg(\n",
        "        F.count(\"order_id\").alias(\"num_orders\"),\n",
        "        F.sum(\"order_amount_usd\").alias(\"total_revenue\"),\n",
        "        F.avg(\"order_amount_usd\").alias(\"avg_order_value\"),\n",
        "    )\n",
        "    .withColumn(\"_aggregated_at\", F.current_timestamp())\n",
        ")\n",
        "gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.gold.revenue_summary\")\n",
        "gold_rows = gold_df.count()\n",
        "gold_time = time.time() - t0\n",
        "print(f\"[GOLD] {gold_rows:,} rows written in {gold_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Persist run metrics\n",
        "total_duration = (datetime.utcnow() - RUN_START).total_seconds()\n",
        "\n",
        "metrics_data = [{\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"job_id\": JOB_ID,\n",
        "    \"run_ts\": RUN_START,\n",
        "    \"bronze_rows\": bronze_rows,\n",
        "    \"silver_rows\": silver_rows,\n",
        "    \"gold_rows\": gold_rows,\n",
        "    \"total_duration_sec\": total_duration,\n",
        "    \"status\": \"success\",\n",
        "}]\n",
        "\n",
        "metrics_schema = StructType([\n",
        "    StructField(\"run_id\", StringType(), False),\n",
        "    StructField(\"job_id\", StringType(), False),\n",
        "    StructField(\"run_ts\", TimestampType(), False),\n",
        "    StructField(\"bronze_rows\", IntegerType(), False),\n",
        "    StructField(\"silver_rows\", IntegerType(), False),\n",
        "    StructField(\"gold_rows\", IntegerType(), False),\n",
        "    StructField(\"total_duration_sec\", FloatType(), False),\n",
        "    StructField(\"status\", StringType(), False),\n",
        "])\n",
        "\n",
        "spark.createDataFrame(metrics_data, schema=metrics_schema).write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"mergeSchema\", \"true\") \\\n",
        "    .saveAsTable(f\"{CATALOG}.gold.pipeline_run_metrics\")\n",
        "\n",
        "print(f\"[METRICS] Run {RUN_ID} recorded. Total duration: {total_duration:.1f}s\")\n",
        "\n",
        "# Task value is set in the next cell for job condition_task (success/failure branching).\n",
        "# Do not exit here so that cell runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "message = \"success\"\n",
        "\n",
        "# Set the message to be used by other tasks\n",
        "dbutils.jobs.taskValues.set(key=\"job_status\", value=message)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
