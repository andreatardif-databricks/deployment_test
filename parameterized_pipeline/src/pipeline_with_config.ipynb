{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameterized DLT Pipeline with External Config File\n",
        "\n",
        "This pipeline demonstrates how to use external YAML configuration files in a DLT pipeline.\n",
        "The configuration file path is passed as a parameter from the DABs bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dlt\n",
        "from pyspark.sql import functions as F\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the src directory to the path so we can import config_loader\n",
        "sys.path.append('/Workspace' + os.path.dirname(os.path.abspath('')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Configuration from YAML File\n",
        "\n",
        "The configuration file path is passed from the pipeline definition in the DABs bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from config_loader import ConfigLoader\n",
        "\n",
        "# Get configuration file path from pipeline settings\n",
        "config_file_path = spark.conf.get(\"config_file_path\", \"../config/dev_config.yml\")\n",
        "catalog = spark.conf.get(\"catalog\", \"main\")\n",
        "schema = spark.conf.get(\"schema\", \"default\")\n",
        "pipeline_config = spark.conf.get(\"pipeline_config\", \"dev\")\n",
        "\n",
        "print(f\"Pipeline Configuration: {pipeline_config}\")\n",
        "print(f\"Config File Path: {config_file_path}\")\n",
        "print(f\"Catalog: {catalog}\")\n",
        "print(f\"Schema: {schema}\")\n",
        "\n",
        "# Load the configuration\n",
        "config = ConfigLoader(config_file_path)\n",
        "config.display_config(mask_secrets=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Access Configuration Values\n",
        "\n",
        "Demonstrate how to access values from the configuration file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Access secrets configuration\n",
        "secret_scope = config.get('secrets.databricks_secret_scope')\n",
        "keyvault_url = config.get('secrets.keyvault_url')\n",
        "\n",
        "print(f\"Secret Scope: {secret_scope}\")\n",
        "print(f\"KeyVault URL: {keyvault_url}\")\n",
        "\n",
        "# Access Kafka configuration\n",
        "kafka_host = config.get('general.kafka.main.host')\n",
        "use_oauth = config.get('general.kafka.main.use_oauth')\n",
        "\n",
        "print(f\"\\nKafka Host: {kafka_host}\")\n",
        "print(f\"Use OAuth: {use_oauth}\")\n",
        "\n",
        "# Access streaming configuration\n",
        "checkpoint_path = config.get('general.streaming.checkpoint_path')\n",
        "consumer_group_prefix = config.get('general.streaming.consumer_group_prefix')\n",
        "\n",
        "print(f\"\\nCheckpoint Path: {checkpoint_path}\")\n",
        "print(f\"Consumer Group Prefix: {consumer_group_prefix}\")\n",
        "\n",
        "# Access OAuth configuration\n",
        "client_id = config.get('general.kafka_oauth.client_id')\n",
        "token_url = config.get('general.kafka_oauth.token_url')\n",
        "\n",
        "print(f\"\\nOAuth Client ID: {client_id}\")\n",
        "print(f\"OAuth Token URL: {token_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bronze Layer - Raw Data Ingestion\n",
        "\n",
        "This table demonstrates using configuration values in a DLT table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dlt.table(\n",
        "    name=\"raw_data_with_config\",\n",
        "    comment=\"Raw data ingestion with configuration\",\n",
        "    table_properties={\n",
        "        \"quality\": \"bronze\",\n",
        "        \"pipeline_config\": pipeline_config,\n",
        "        \"kafka_host\": kafka_host,\n",
        "        \"consumer_group\": consumer_group_prefix\n",
        "    }\n",
        ")\n",
        "def bronze_raw_data_with_config():\n",
        "    \"\"\"Ingest raw data using configuration parameters\"\"\"\n",
        "    return (\n",
        "        spark.range(0, 100)\n",
        "        .withColumn(\"name\", F.concat(F.lit(\"user_\"), F.col(\"id\")))\n",
        "        .withColumn(\"value\", F.rand() * 100)\n",
        "        .withColumn(\"timestamp\", F.current_timestamp())\n",
        "        .withColumn(\"config\", F.lit(pipeline_config))\n",
        "        .withColumn(\"kafka_host\", F.lit(kafka_host))\n",
        "        .withColumn(\"consumer_group\", F.lit(consumer_group_prefix))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Silver Layer - Data Processing with Configuration\n",
        "\n",
        "This table applies transformations using configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dlt.table(\n",
        "    name=\"processed_data_with_config\",\n",
        "    comment=\"Processed data using configuration parameters\",\n",
        "    table_properties={\n",
        "        \"quality\": \"silver\",\n",
        "        \"pipeline_config\": pipeline_config,\n",
        "        \"checkpoint_path\": checkpoint_path\n",
        "    }\n",
        ")\n",
        "@dlt.expect_or_drop(\"valid_id\", \"id IS NOT NULL\")\n",
        "@dlt.expect_or_drop(\"valid_value\", \"value >= 0\")\n",
        "def silver_processed_data_with_config():\n",
        "    \"\"\"Process data with quality checks\"\"\"\n",
        "    return (\n",
        "        dlt.read(\"raw_data_with_config\")\n",
        "        .withColumn(\"value_rounded\", F.round(F.col(\"value\"), 2))\n",
        "        .withColumn(\"value_category\", \n",
        "            F.when(F.col(\"value\") < 33, \"low\")\n",
        "            .when(F.col(\"value\") < 66, \"medium\")\n",
        "            .otherwise(\"high\")\n",
        "        )\n",
        "        .withColumn(\"processed_timestamp\", F.current_timestamp())\n",
        "        .withColumn(\"checkpoint_path\", F.lit(checkpoint_path))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example: Retrieve Secrets from Key Vault\n",
        "\n",
        "This cell demonstrates how to retrieve secrets using the configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Retrieve a secret from Databricks Secret Scope\n",
        "# Uncomment and use this when you have actual secrets configured\n",
        "\n",
        "# secret_key_name = config.get('general.kafka_oauth.client_secret_vault_key')\n",
        "# if secret_key_name:\n",
        "#     try:\n",
        "#         client_secret = config.get_secret_from_scope(secret_key_name)\n",
        "#         print(f\"✓ Successfully retrieved secret: {secret_key_name}\")\n",
        "#         # Use the secret in your pipeline\n",
        "#     except Exception as e:\n",
        "#         print(f\"✗ Failed to retrieve secret: {e}\")\n",
        "\n",
        "print(\"Note: Secret retrieval example is commented out. Uncomment when you have secrets configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold Layer - Aggregated Metrics\n",
        "\n",
        "Create aggregated data with configuration metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dlt.table(\n",
        "    name=\"aggregated_data_with_config\",\n",
        "    comment=\"Aggregated business metrics with configuration metadata\",\n",
        "    table_properties={\n",
        "        \"quality\": \"gold\",\n",
        "        \"pipeline_config\": pipeline_config,\n",
        "        \"kafka_host\": kafka_host\n",
        "    }\n",
        ")\n",
        "def gold_aggregated_data_with_config():\n",
        "    \"\"\"Create aggregated metrics\"\"\"\n",
        "    return (\n",
        "        dlt.read(\"processed_data_with_config\")\n",
        "        .groupBy(\"value_category\", \"config\", \"kafka_host\")\n",
        "        .agg(\n",
        "            F.count(\"*\").alias(\"record_count\"),\n",
        "            F.avg(\"value_rounded\").alias(\"avg_value\"),\n",
        "            F.min(\"value_rounded\").alias(\"min_value\"),\n",
        "            F.max(\"value_rounded\").alias(\"max_value\"),\n",
        "            F.stddev(\"value_rounded\").alias(\"stddev_value\")\n",
        "        )\n",
        "        .withColumn(\"aggregation_timestamp\", F.current_timestamp())\n",
        "        .withColumn(\"environment\", F.lit(pipeline_config))\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
