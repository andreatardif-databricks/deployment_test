{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a626959-61c8-4bba-84d2-2a4ecab1f7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DLT pipeline\n",
    "\n",
    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/dlt_incremental_refresh_demo.pipeline.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DLT and src/dlt_incremental_refresh_demo\n",
    "import dlt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add widgets for catalog and schema parameters\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"\", \"Schema Name\")\n",
    "\n",
    "# Get parameter values (fallback to defaults if not provided)\n",
    "catalog = dbutils.widgets.get(\"catalog\") or spark.conf.get(\"bundle.var.catalog\", \"main\")\n",
    "schema = dbutils.widgets.get(\"schema\") or spark.conf.get(\"bundle.var.schema\", \"incremental_dlt\")\n",
    "\n",
    "sys.path.append(spark.conf.get(\"bundle.sourcePath\", \".\"))\n",
    "from pyspark.sql.functions import expr\n",
    "# from dlt_incremental_refresh_demo import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc19dba-61fd-4a89-8f8c-24fee63bfb14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def name_group_1():\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            name,\n",
    "            COUNT(id) AS name_count\n",
    "        FROM {catalog}.{schema}.random_data\n",
    "        GROUP BY name; \"\"\")\n",
    "\n",
    "@dlt.table\n",
    "@dlt.expect_all_or_drop({\"valid_customer_name\" : \"name <> 'Alice'\"})\n",
    "def filtered_name():\n",
    "    return dlt.read(\"name_group_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def name_group_2():\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            name,\n",
    "            CURRENT_TIMESTAMP() AS load_time,\n",
    "            COUNT(id) AS name_count\n",
    "        FROM {catalog}.{schema}.random_data\n",
    "        GROUP BY ALL; \"\"\")\n",
    "\n",
    "@dlt.table\n",
    "def full_table():\n",
    "    return spark.sql(f\"\"\"SELECT * FROM {catalog}.{schema}.random_data;\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
