{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"main\"\n",
    "schema = \"incremental_dlt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client and auth\n",
    "w = WorkspaceClient()\n",
    "pipelines = w.pipelines.list_pipelines()\n",
    "token_value = w.tokens.create(comment=f\"sdk-{time.time_ns()}\").token_value\n",
    "host = w.config.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token_value}\"\n",
    "}\n",
    "\n",
    "# Worker function to fetch events for a pipeline\n",
    "def fetch_events(pipeline):\n",
    "    pipeline_id = pipeline.pipeline_id\n",
    "    url = f\"{host}/api/2.0/pipelines/{pipeline_id}/events\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        events = response.json().get(\"events\", [])\n",
    "        if not events:\n",
    "            return []  # Skip if no events\n",
    "\n",
    "        # Tag events with pipeline info\n",
    "        for event in events:\n",
    "            event[\"pipeline_id\"] = pipeline_id\n",
    "            event[\"pipeline_name\"] = pipeline.name\n",
    "\n",
    "        print(f\"Fetched {len(events)} events for {pipeline.name}\")\n",
    "        return events\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {pipeline.name}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED\n",
    "import itertools\n",
    "\n",
    "max_threads = 3\n",
    "eventful_pipeline_limit = 1200\n",
    "eventful_pipeline_count = 0\n",
    "all_events = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
    "    # Generator to control submission\n",
    "    pipeline_iter = iter(pipelines)\n",
    "    running_futures = []\n",
    "\n",
    "    while eventful_pipeline_count < eventful_pipeline_limit:\n",
    "        # Fill up the thread pool\n",
    "        while len(running_futures) < max_threads:\n",
    "            try:\n",
    "                p = next(pipeline_iter)\n",
    "                future = executor.submit(fetch_events, p)\n",
    "                running_futures.append(future)\n",
    "            except StopIteration:\n",
    "                break  # No more pipelines to submit\n",
    "\n",
    "        # Wait for any future to complete\n",
    "        done, _ = wait(running_futures, return_when=FIRST_COMPLETED)\n",
    "        for future in done:\n",
    "            running_futures.remove(future)\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_events.extend(result)\n",
    "                eventful_pipeline_count += 1\n",
    "                if eventful_pipeline_count >= eventful_pipeline_limit:\n",
    "                    break\n",
    "\n",
    "print(f\"\\nCollected events from {eventful_pipeline_count} pipelines in {round(time.time() - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "all_pipelines_df = pd.DataFrame(all_events)\n",
    "len(all_pipelines_df['pipeline_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(all_pipelines_df)\n",
    "spark_df.createOrReplaceTempView(\"demo_event_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT id, sequence, origin, timestamp, message, level, event_type, maturity_level, pipeline_id, pipeline_name, error FROM demo_event_log\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{schema}.raw_event_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "with refresh_method AS (\n",
    "  SELECT \n",
    "      origin.org_id, \n",
    "      origin.pipeline_id,\n",
    "      timestamp, \n",
    "      origin.pipeline_name, \n",
    "      REGEXP_EXTRACT(message, 'executed as ([A-Z_]+)', 1) AS refresh_type,\n",
    "\n",
    "      -- maintenance type\n",
    "      CASE \n",
    "          WHEN get(details.planning_information.technique_information, 0).is_chosen = true THEN get(details.planning_information.technique_information, 0).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 1).is_chosen = true THEN get(details.planning_information.technique_information, 1).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 2).is_chosen = true THEN get(details.planning_information.technique_information, 2).maintenance_type \n",
    "          WHEN get(details.planning_information.technique_information, 3).is_chosen = true THEN get(details.planning_information.technique_information, 3).maintenance_type \n",
    "          ELSE NULL \n",
    "      END AS chosen_maintenance_type,\n",
    "\n",
    "      -- cost\n",
    "      CASE \n",
    "          WHEN get(details.planning_information.technique_information, 0).is_chosen = true THEN get(details.planning_information.technique_information, 0).cost\n",
    "          WHEN get(details.planning_information.technique_information, 1).is_chosen = true THEN get(details.planning_information.technique_information, 1).cost\n",
    "          WHEN get(details.planning_information.technique_information, 2).is_chosen = true THEN get(details.planning_information.technique_information, 2).cost\n",
    "          WHEN get(details.planning_information.technique_information, 3).is_chosen = true THEN get(details.planning_information.technique_information, 3).cost\n",
    "          ELSE NULL \n",
    "      END AS cost,\n",
    "\n",
    "      -- recompute reason\n",
    "      CASE \n",
    "          WHEN (CASE \n",
    "                  WHEN get(details.planning_information.technique_information, 0).is_chosen = true THEN get(details.planning_information.technique_information, 0).maintenance_type \n",
    "                  WHEN get(details.planning_information.technique_information, 1).is_chosen = true THEN get(details.planning_information.technique_information, 1).maintenance_type \n",
    "                  WHEN get(details.planning_information.technique_information, 2).is_chosen = true THEN get(details.planning_information.technique_information, 2).maintenance_type \n",
    "                  WHEN get(details.planning_information.technique_information, 3).is_chosen = true THEN get(details.planning_information.technique_information, 3).maintenance_type \n",
    "                  ELSE NULL \n",
    "                END) = 'MAINTENANCE_TYPE_COMPLETE_RECOMPUTE' THEN \n",
    "              COALESCE(\n",
    "                  CASE WHEN get(get(details.planning_information.technique_information, 1).incrementalization_issues, 0).issue_type = 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "                       THEN 'EXPECTATIONS_NOT_SUPPORTED' ELSE NULL END,\n",
    "                  CASE WHEN get(get(details.planning_information.technique_information, 2).incrementalization_issues, 0).issue_type = 'EXPECTATIONS_NOT_SUPPORTED' \n",
    "                       THEN 'EXPECTATIONS_NOT_SUPPORTED' ELSE NULL END,\n",
    "                  get(get(details.planning_information.technique_information, 0).incrementalization_issues, 0).issue_type,\n",
    "                  get(get(details.planning_information.technique_information, 1).incrementalization_issues, 0).issue_type,\n",
    "                  get(get(details.planning_information.technique_information, 2).incrementalization_issues, 0).issue_type,\n",
    "                  get(get(details.planning_information.technique_information, 3).incrementalization_issues, 0).issue_type,\n",
    "                  'UNKNOWN_ISSUE'\n",
    "              )\n",
    "          ELSE 'incremental recompute'\n",
    "      END AS recompute_reason\n",
    "  FROM demo_event_log\n",
    "  WHERE event_type = 'planning_information'\n",
    "),\n",
    "\n",
    "flow_info AS (\n",
    "  SELECT \n",
    "    origin.org_id, \n",
    "    details.flow_definition.output_dataset as output_dataset,\n",
    "    details.flow_definition.explain_text as query,\n",
    "    details.flow_definition.flow_type as table_type\n",
    "  FROM demo_event_log\n",
    "  WHERE event_type = 'flow_definition'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    r.timestamp,\n",
    "    r.pipeline_id,\n",
    "    r.pipeline_name,\n",
    "    r.refresh_type,\n",
    "    r.chosen_maintenance_type,\n",
    "    r.recompute_reason,\n",
    "    r.cost,\n",
    "    f.output_dataset, \n",
    "    f.query, \n",
    "    f.table_type\n",
    "FROM refresh_method r\n",
    "LEFT JOIN flow_info f ON r.org_id = f.org_id\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.{schema}.combined_event_log\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
